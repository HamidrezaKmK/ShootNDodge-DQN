{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "930dfb40",
   "metadata": {},
   "source": [
    "# Shoot N Dodge\n",
    "\n",
    "In this assignment, you will be programming an agent to be able to play a new Atari game called *ShootNDodge*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fa016",
   "metadata": {},
   "source": [
    "## Game description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dabb5",
   "metadata": {},
   "source": [
    "\n",
    "## Installing the game\n",
    "\n",
    "The game is provided as a python package and can be run as an **OpenAI-Gym** environment. To do so, you will have to go to the setup folder and install the package on your python virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3e0bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/hamidreza/myprojects/ShootNDodge-DQN/gym-shootndodge\n",
      "Requirement already satisfied: gym in /home/hamidreza/anaconda3/lib/python3.8/site-packages (from gym-shootndodge==0.0.94) (0.21.0)\n",
      "Requirement already satisfied: opencv-python in /home/hamidreza/anaconda3/lib/python3.8/site-packages (from gym-shootndodge==0.0.94) (4.5.5.62)\n",
      "Requirement already satisfied: pillow in /home/hamidreza/anaconda3/lib/python3.8/site-packages (from gym-shootndodge==0.0.94) (8.3.1)\n",
      "Requirement already satisfied: numpy in /home/hamidreza/anaconda3/lib/python3.8/site-packages (from gym-shootndodge==0.0.94) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/hamidreza/anaconda3/lib/python3.8/site-packages (from gym->gym-shootndodge==0.0.94) (1.6.0)\n",
      "Installing collected packages: gym-shootndodge\n",
      "  Attempting uninstall: gym-shootndodge\n",
      "    Found existing installation: gym-shootndodge 0.0.93\n",
      "    Uninstalling gym-shootndodge-0.0.93:\n",
      "      Successfully uninstalled gym-shootndodge-0.0.93\n",
      "  Running setup.py develop for gym-shootndodge\n",
      "Successfully installed gym-shootndodge\n"
     ]
    }
   ],
   "source": [
    "!pip install -e gym-shootndodge/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279e101",
   "metadata": {},
   "source": [
    "Now you can easily create the environment similar to any other gym environment using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d0e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'shootndodge-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae2f8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_shootndodge\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "obs = env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'), aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d264239",
   "metadata": {},
   "source": [
    "Run the game in frames to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fcfbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(2022)\n",
    "obs = env.reset()\n",
    "a = 2\n",
    "for i in range(5000):\n",
    "    if (i + 1) % 40 == 0:\n",
    "        a = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        plt.imshow(screen, aspect='auto')\n",
    "        plt.show()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb31477",
   "metadata": {},
   "source": [
    "### Let's play a little\n",
    "\n",
    "Pay attention to zoom and play function. Control: Up arrow, Down arrow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c79dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # does not work in Colab.\n",
    "# # make keyboard interrupt to continue\n",
    "\n",
    "# from gym.utils.play import play\n",
    "\n",
    "# play(env=gym.make(ENV_NAME), zoom=5, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce6b68",
   "metadata": {},
   "source": [
    "### Preprocessing game image\n",
    "\n",
    "The raw Atari image is large, 504x510x3 by default. However, we don't need that level of detail in order to learn them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "\n",
    "Resizing to a smaller shape, 64 x 64\n",
    "Converting to grayscale\n",
    "Cropping irrelevant image parts (top, bottom and edges)\n",
    "Also please keep one dimension for channel so that final shape would be 1 x 64 x 64.\n",
    "\n",
    "Tip: You can implement your own grayscale converter and assign a huge weight to the red channel. This dirty trick is not necessary but it will speed up learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5683cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "import cv2\n",
    "\n",
    "badblock_colors = [(200, 200, 0), (153, 76, 0), (204, 0, 0)]\n",
    "\n",
    "class PreprocessAtariObs(ObservationWrapper):\n",
    "    def __init__(self, env, gray_scale=False):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.img_size = (64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, (64, 64, 1 if gray_scale else env.observation_space.shape[2]))\n",
    "\n",
    "        self.gray_scale = gray_scale\n",
    "        \n",
    "    def _to_gray_scale(self, rgb, channel_weights=[0.6, 0.3, 0.1]):\n",
    "        gray_img = np.zeros((rgb.shape[0], rgb.shape[1], 1))\n",
    "        for i in range(rgb.shape[0]):\n",
    "            for j in range(rgb.shape[1]):\n",
    "                gray_img[i, j, 0] = channel_weights[0] * rgb[i,j,0] + channel_weights[1] * rgb[i,j,1] + channel_weights[2] * rgb[i,j,2]\n",
    "        return gray_img\n",
    "\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        # Here's what you need to do:\n",
    "        #  * crop image, remove irrelevant parts\n",
    "        #  * resize image to self.img_size\n",
    "        #     (use imresize from any library you want,\n",
    "        #      e.g. opencv, skimage, PIL, keras)\n",
    "        #  * cast image to grayscale\n",
    "        #  * convert image pixels to (0,1) range, float32 type\n",
    "        #print(\"processing obs\")\n",
    "        img_cropped = img[39:504-39, 39:510-39]\n",
    "        processed_img = cv2.resize(img_cropped.astype(np.float32), self.img_size)\n",
    "        #print(\"HIO\")\n",
    "        if self.gray_scale:\n",
    "            print(\"WHAT!\")\n",
    "            processed_img = self._to_gray_scale(processed_img)\n",
    "        processed_img = (processed_img - processed_img.min()) / processed_img.max()\n",
    "        #print(\"CHIZ\", gray_img.shape)\n",
    "#         pooling_window = 7\n",
    "#         sr, sc = gray_img.shape[0] // pooling_window + 1, gray_img.shape[1] // pooling_window + 1\n",
    "#         resized_img = np.zeros((sr, sc))\n",
    "#         for i in range(sr):\n",
    "#             for j in range(sc):\n",
    "#                 rL = i * pooling_window\n",
    "#                 rR = min(rL + pooling_window, gray_img.shape[0])\n",
    "#                 cL = j * pooling_window\n",
    "#                 cR = min(cL + pooling_window, gray_img.shape[1])\n",
    "#                 block = gray_img[rL:rR,cL:cR]\n",
    "#                 resized_img[i,j] = block.max()\n",
    "        #print(\"processing obs DONE!\")\n",
    "        #print(resized_img.shape, \"SHAPE\")\n",
    "        \n",
    "        #processed_img = (gray_img / gray_img.max()).astype(np.float32)\n",
    "        \n",
    "        #print(resized_img)\n",
    "        #print(resized_img.shape)\n",
    "        return processed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4552e",
   "metadata": {},
   "source": [
    "Now we run the code below to check whether the wrapper is ok or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ac7191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "Formal tests seem fine.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "# spawn game instance for tests\n",
    "env = gym.make(ENV_NAME)  # create raw env\n",
    "env = PreprocessAtariObs(env)\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "env.reset()\n",
    "obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "print(obs.shape)\n",
    "# test observation\n",
    "assert obs.ndim == 3, \"observation must be [h, w, c] even in grayscale mode\"\n",
    "# assert obs.shape == observation_shape\n",
    "# assert obs.dtype == 'float32'\n",
    "# assert 0 <= np.min(obs) and np.max(\n",
    "#     obs) <= 1, \"convert image pixels to [0,1] range\"\n",
    "\n",
    "print(\"Formal tests seem fine.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa571140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54886/2830843124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(observation.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(observation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojects/ShootNDodge-DQN/gym-shootndodge/gym_shootndodge/envs/shootndodge_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprv_kill_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhit_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprv_hit_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"additional_info\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojects/ShootNDodge-DQN/gym-shootndodge/gym_shootndodge/envs/shootndodge_env.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace_ship\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        self.space_ship.h, self.space_ship.ship_color)\n\u001b[0;32m--> 226\u001b[0;31m         self.draw_rect(self.space_ship.offset + 2 * self.space_ship.d, self.space_ship.low_end, self.space_ship.d,\n\u001b[0m\u001b[1;32m    227\u001b[0m                        self.space_ship.h, self.space_ship.ship_color)\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# DRAW BULLETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojects/ShootNDodge-DQN/gym-shootndodge/gym_shootndodge/envs/shootndodge_env.py\u001b[0m in \u001b[0;36mdraw_rect\u001b[0;34m(self, sX, sY, LX, LY, color)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBORDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBORDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "obs = env.reset()\n",
    "a = 0\n",
    "for i in range(1000):\n",
    "    if (i + 1) % 40 == 0:\n",
    "        a = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    #print(observation.shape)\n",
    "    #print(observation)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        #print(observation.shape)\n",
    "        #plt.imshow(observation.astype(np.int32))\n",
    "        plt.imshow(observation, interpolation='none', aspect='auto')\n",
    "        plt.show()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d256a3",
   "metadata": {},
   "source": [
    "Store frames in frame buffer:\n",
    "\n",
    "-- PROBLEM: the image seems flashy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22288d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "from gym.core import Wrapper\n",
    "\n",
    "# TODO: Maybe consequtive frames are not the best because they don't capture movement as much\n",
    "\n",
    "class FrameBuffer(Wrapper):\n",
    "    def __init__(self, env, n_frames=4, frame_batch_size=1):\n",
    "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
    "        \n",
    "        assert n_frames % frame_batch_size == 0, \"number of frames shoud be dividable by frame_batch_size\"\n",
    "        \n",
    "        super(FrameBuffer, self).__init__(env)\n",
    "        height, width, n_channels= env.observation_space.shape\n",
    "        obs_shape = (height, width, n_channels * n_frames // frame_batch_size)\n",
    "        frame_buffer_shape = (height, width, n_channels * n_frames)\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.framebuffer = np.zeros(frame_buffer_shape, 'float32')\n",
    "        self.compressed_buffer = np.zeros(obs_shape, 'float32')\n",
    "        self.frame_batch_size = frame_batch_size\n",
    "        self.cyclic = frame_batch_size\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
    "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
    "        self.update_buffer(self.env.reset())\n",
    "        return self.compressed_buffer\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
    "        new_img, reward, done, info = self.env.step(action)\n",
    "        self.update_buffer(new_img)\n",
    "        return self.compressed_buffer, reward, done, info\n",
    "\n",
    "    def update_buffer(self, img):\n",
    "        offset = self.env.observation_space.shape[2]\n",
    "        axis = 2\n",
    "        cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
    "        self.framebuffer = np.concatenate(\n",
    "            [img, cropped_framebuffer], axis=axis)\n",
    "        \n",
    "        if self.frame_batch_size == 1:\n",
    "            self.compressed_buffer = self.framebuffer\n",
    "        else:    \n",
    "            self.cyclic -= 1\n",
    "            if self.cyclic == 0:\n",
    "                self.cyclic = self.frame_batch_size\n",
    "                self.compressed_buffer = self.compressed_buffer[:,:,:-offset]\n",
    "                batch_section = self.framebuffer[:,:,:offset*self.cyclic]\n",
    "                picker = offset * np.arange(self.cyclic)\n",
    "                for i in range(offset):\n",
    "                    #print((offset - i - 1) + picker)\n",
    "                    avg = np.expand_dims(np.mean(batch_section[:,:,(offset - i - 1) + picker], axis=2), 2)\n",
    "\n",
    "                    self.compressed_buffer = np.concatenate(\n",
    "                        [avg, self.compressed_buffer], axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592a7a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(64, 64, 12)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = PreprocessAtariObs(env)\n",
    "env = FrameBuffer(env)\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(state_shape)\n",
    "s = env.reset()\n",
    "# s, _, _, _ = env.step(env.action_space.sample())\n",
    "# s, _, _, _ = env.step(env.action_space.sample())\n",
    "# print(\"HO1\")\n",
    "# s, _, _, _ = env.step(env.action_space.sample())\n",
    "# print(\"HO2\")\n",
    "# s, _, _, _ = env.step(env.action_space.sample())\n",
    "a = 0\n",
    "for i in range(1000):\n",
    "    if (i + 1) % 5 == 0:\n",
    "        a = env.action_space.sample()\n",
    "    s, _, done, _ = env.step(a)\n",
    "    if done:\n",
    "        break\n",
    "    if (i + 1) % 20 == 0:\n",
    "        clear_output(True)\n",
    "        for f in range(4):\n",
    "            plt.imshow(s[:,:,f * 3:f * 3 + 3], interpolation='none', aspect='auto')\n",
    "            plt.show()\n",
    "print(n_actions)\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc491dc",
   "metadata": {},
   "source": [
    "### The DQN architecture\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
    "\n",
    "You can build any architecture you want, but for reference, here's something that will more or less work:\n",
    "\n",
    "(HERE SHOULD BE A PHOTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14091188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4013915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, env, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.state_shape = (env.observation_space.shape[2],\n",
    "                            env.observation_space.shape[0],\n",
    "                            env.observation_space.shape[1])\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.conv1 = nn.Conv2d(self.state_shape[0], 16, (3, 3), 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), 2)\n",
    "        self.linear = self.linear1 = nn.Linear(3136, self.n_actions)\n",
    "        \n",
    "        # TODO: one RELU for all?!\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Flatten = nn.Flatten()\n",
    "        \n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        # nn.Flatten() can be useful\n",
    "        #<YOUR CODE>\n",
    "        #self.layers = nn.Sequential(\n",
    "            \n",
    "        #)\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, h, w, 4 * c]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        state_t = state_t.transpose(3, 2)\n",
    "        state_t = state_t.transpose(2, 1)\n",
    "        state_t = self.ReLU(self.conv1(state_t))\n",
    "        state_t = self.ReLU(self.conv2(state_t))\n",
    "        state_t = self.ReLU(self.conv3(state_t))\n",
    "        qvalues = self.ReLU(self.linear(self.Flatten(state_t)))\n",
    "        \n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == self.n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee68ea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape: (64, 64, 12)\n",
      "Action space count: 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env = PreprocessAtariObs(env)\n",
    "env = FrameBuffer(env)\n",
    "print('Observation space shape: {}\\nAction space count: {}'.format(env.observation_space.shape, env.action_space.n))\n",
    "agent = DQNAgent(env, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d621b4",
   "metadata": {},
   "source": [
    "Run the agent to see if it encounters any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf379ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, visualize=False):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in trange(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for t in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            if visualize and (t + 1) % 20 == 0:\n",
    "                clear_output(True)\n",
    "                plt.imshow(s[:,:,0:3], interpolation='none', aspect='auto')\n",
    "                plt.show()   \n",
    "                #print(reward)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56bcddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                         | 1/15 [00:26<06:07, 26.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54886/1792156355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_54886/182557422.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(env, agent, n_games, greedy, t_max, visualize)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54886/1944197674.py\u001b[0m in \u001b[0;36mget_qvalues\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m         \u001b[0mmodel_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, n_games=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b83e96",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for batch_size random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02d5a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is shamelessly stolen from\n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        # fill the data cyclically, if the list is not yet complete append it\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return (\n",
    "            np.array(obses_t),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(obses_tp1),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [\n",
    "            random.randint(0, len(self._storage) - 1)\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "        return self._encode_sample(idxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26163fd0",
   "metadata": {},
   "source": [
    "Check whether the Replay Buffer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a65ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
    "                   1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f63f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6819676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for i in range(n_steps):\n",
    "        q_values = agent.get_qvalues([s])\n",
    "        a = agent.sample_actions(q_values)[0]\n",
    "        s_, rwd, done, info = env.step(a)\n",
    "        sum_rewards += rwd\n",
    "        exp_replay.add(s, a, rwd, s_, done)\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = s_\n",
    "            \n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddb006c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
    "                                 \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done should be strictly True or False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9131c23",
   "metadata": {},
   "source": [
    "How to use target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dd6b4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network = DQNAgent(env, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500c3ab",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef092912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = predicted_next_qvalues.max(-1)[0]\n",
    "\n",
    "    assert next_state_values.dim(\n",
    "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = rewards + is_not_done * gamma * next_state_values\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10559dba",
   "metadata": {},
   "source": [
    "Sanity checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "964f083b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "loss must be differentiable w.r.t. network weights",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35740/1446038081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m assert loss.requires_grad and tuple(loss.data.size()) == (\n\u001b[1;32m     10\u001b[0m     ), \"you must return scalar loss - mean over batch\"\n\u001b[0;32m---> 11\u001b[0;31m assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n\u001b[0m\u001b[1;32m     12\u001b[0m               0), \"loss must be differentiable w.r.t. network weights\"\n\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target network should not have grads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: loss must be differentiable w.r.t. network weights"
     ]
    }
   ],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
    "    ), \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946cf59",
   "metadata": {},
   "source": [
    "## Main Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf843130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b1aea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcdfdd80970>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 2022#<YOUR CODE: your favourite random seed>\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d72d0384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 12)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env.seed(seed)\n",
    "env = PreprocessAtariObs(env)\n",
    "env = FrameBuffer(env)\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "print(state_shape)\n",
    "print(n_actions)\n",
    "state = env.reset()\n",
    "agent = DQNAgent(env, epsilon=1).to(device)\n",
    "target_network = DQNAgent(env).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36f656",
   "metadata": {},
   "source": [
    "Buffer of size  104  fits into 5 Gb RAM.\n",
    "\n",
    "Larger sizes ( 105  and  106  are common) can be used. It can improve the learning, but  104  is quiet enough.  102  will probably fail learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff321a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def is_enough_ram(min_available_gb=0.1):\n",
    "    mem = psutil.virtual_memory()\n",
    "    return mem.available >= min_available_gb * (1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d07e8a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 99/100 [02:49<00:01,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(10**4)\n",
    "for i in tqdm(range(100)):\n",
    "    if not is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78174d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 16\n",
    "total_steps = 3 * 10**6\n",
    "decay_steps = 10**6\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 50\n",
    "refresh_target_network_freq = 5000\n",
    "eval_freq = 5000\n",
    "\n",
    "max_grad_norm = 50\n",
    "\n",
    "n_lives = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d080ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd010c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve, gaussian\n",
    "\n",
    "def linear_decay(init_val, final_val, cur_step, total_steps):\n",
    "    if cur_step >= total_steps:\n",
    "        return final_val\n",
    "    return (init_val * (total_steps - cur_step) +\n",
    "            final_val * cur_step) / total_steps\n",
    "\n",
    "def smoothen(values):\n",
    "    kernel = gaussian(100, std=100)\n",
    "    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e5d6709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 2286/2999861 [01:05<23:49:45, 34.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35740/3219533702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35740/1588500503.py\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m(states, actions, rewards, next_states, is_done, agent, target_network, gamma, check_shapes, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# get q-values for all actions in current states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpredicted_qvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# compute q-values for all actions in next states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35740/1944197674.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state_t)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for step in trange(step, total_steps + 1):\n",
    "    if not is_enough_ram():\n",
    "        print('less that 100 Mb RAM available, freezing')\n",
    "        print('make sure everythin is ok and make KeyboardInterrupt to continue')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "    #print(\"epsilon = {}\".format(agent.epsilon))\n",
    "    # play\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "    #print(\"Played!\")\n",
    "    # train\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
    "    loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device)\n",
    "    #print(\"Calculated Loss\")\n",
    "    #<YOUR CODE: sample batch_size of data from experience replay>\n",
    "\n",
    "    #loss = <YOUR CODE: compute TD loss>\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    #print(\"Backprop done!\")\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        #target_network = DQNAgent(env).to(device)\n",
    "        target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        env = gym.make(ENV_NAME)\n",
    "        env.seed(seed)\n",
    "        env = PreprocessAtariObs(env)\n",
    "        env = FrameBuffer(env)\n",
    "        print(\"Started evaluating\")\n",
    "        print(\"start rw history\")\n",
    "        n_lives = 1\n",
    "        mean_rw_history.append(evaluate(\n",
    "            env, agent, n_games=3 * n_lives, greedy=True)\n",
    "        )\n",
    "        \n",
    "        env = gym.make(ENV_NAME)\n",
    "        env.seed(seed)\n",
    "        env = PreprocessAtariObs(env)\n",
    "        env = FrameBuffer(env)\n",
    "        print(\"Start init 'q' values\")\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [env.reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward per life\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Initial state V\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
