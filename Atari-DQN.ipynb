{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "930dfb40",
   "metadata": {},
   "source": [
    "# Mastering Atari Games using Deep Reinforcement Learning\n",
    "\n",
    "In this assignment, you will learn how to implement a Deep-Q-Network (DQN), and to demonstrate that, you may implement and train a DQN and run it on an Atari game in **OpenAI-Gym** environment. Additionally, you may also want to refer to the [original paper](https://arxiv.org/pdf/1312.5602.pdf) regarding the usage of DQN in Atari networks.\n",
    "\n",
    "For this assignment, you will have two options to choose from:\n",
    "* **Breakout**: A famous Atari game that is a staple for learning deep q networks. This game is already available in the gym library environment. By solving this completely you will receive 100/100 points.\n",
    "* **Shoot-N-Dodge**: This is another Atari game that has been developed for the sole reason of this particular assignment. The game involves a spaceship with guns and a bunch of meteors coming your way! For those of you who are up for the challenge, you may run your DQN on this game and receive **extra points**. By implementing and training a network capable of solving this custom game you will receive 150/100 points with 50 points being extra.\n",
    "\n",
    "In the following, you can read the description of the games and how to start running it. We advise using either `Google-Colab` or `Kaggle` for training the model as DQN models usually take a lot of time to train.\n",
    "\n",
    "Before we start, make sure to install and import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c56468",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv\n",
    "!pip install gym\n",
    "!pip install opencv-python\n",
    "!pip install pillow\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning requirements\n",
    "### make sure to use gpu for this assignment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e8587",
   "metadata": {},
   "source": [
    "As alluded, we will use the `gym` framework to simulate our game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fa016",
   "metadata": {},
   "source": [
    "## Option 1: Breakout\n",
    "\n",
    "The famous game [Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)) involves a paddle and bricks. It is one of the games in which DQNs have proven to work very good on. \n",
    "\n",
    "<img src=\"figs/nerd.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if you intend to implement the breakout option\n",
    "\n",
    "ENV_NAME = \"BreakoutNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87403c",
   "metadata": {},
   "source": [
    "## Option 2: Shoot-N-Dodge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b94461",
   "metadata": {},
   "source": [
    "In this game you control a space ship that shoots bullets. At each frame, you may either move up, down, or stay still. Every once in a while some meteors colored in red show up at random at the other end of the screen and you should avoid or destroy them. If either one of these meteors hits you, you will die!\n",
    "\n",
    "To destroy the meteors, you have to shoot them three times and each time you shoot them their color changes until they disappear. They change color from red to orange to yellow and then they are gone.\n",
    "\n",
    "<img src=\"figs/shoot-n-dodge.png\" width=430 height=420 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if you intend to implement the ShootNDodge option\n",
    "\n",
    "ENV_NAME = 'shootndodge-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dabb5",
   "metadata": {},
   "source": [
    "\n",
    "### Installing the game\n",
    "\n",
    "The game is provided as a python package and can be run as an **OpenAI-Gym** environment. To do so, you will have to go to the setup folder and install the package on your python virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# .... You should be at the root directory of the repository in order to run this\n",
    "\n",
    "!pip install -e gym-shootndodge/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea45f08",
   "metadata": {},
   "source": [
    "## Seeing the game\n",
    "\n",
    "Without further ado, we shall start see how your game works. To do so, run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "obs = env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'), aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d264239",
   "metadata": {},
   "source": [
    "Run the game in multiple frames to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env.seed(2022)\n",
    "obs = env.reset()\n",
    "\n",
    "a = env.action_space.sample()\n",
    "for i in range(5000):\n",
    "    if (i + 1) % 40 == 0:\n",
    "        a = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        plt.imshow(screen, aspect='auto')\n",
    "        plt.show()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d576c1d",
   "metadata": {},
   "source": [
    "Feel free to hard code any policies before moving on to the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e3b37",
   "metadata": {},
   "source": [
    "## Environment Wrappers\n",
    "\n",
    "In this section, we will do some preprocessings on the environment. Basically our work consists of wrapping our environment around three wrappers. You should implement each of these wrappers in the `src/wrappers.py` file provided to you in the assignment folder. \n",
    "* Reducing image input size: The Atari game given as input has high resolution which is definitely not needed for the model and it can learn efficiently without all of the additional data. These additional pixels create overhead. We can thus save a lot of time by preprocessing game image, including:\n",
    "    - Resizing to a smaller shape, (64 x 64 recommended for the Breakout and 50 x 50 recommended for ShootNDodge)\n",
    "    - Converting to grayscale (Not recommended for ShootNDodge)\n",
    "    - Cropping irrelevant image parts (top, bottom and edges)\n",
    "    - Also please keep one dimension for channel even if the image is grayscale so that final shape would be (H x W x C)\n",
    "* Reward clipping: There might be the case that many of the RL problems have unstable rewards. In case the rewards are not continuous with respect to the state space, then the local optimization methods used for learning the network will fall short. To address this issue we create a wrapper around our original environment - Note that this is not a necessity and in case you feel comfortable with the rewards given in the original environment, then your wrapper should simply return the previous values as if nothing has heppened! Feel free to accumulate more knowledge about reward clipping where we reduce the range of the rewards to [-1, 1]. In case you observe increasing gradient norms in the learning process, then it would be good to consider this method.\n",
    "* Remove Blinking: Since we resize the image, we might come accross blinking whereas in one frame a certain thing is visible, but at the split-frame afterward, it fades and then returns again. To remove this issue we introduce a `MaxAndSkipEnv` in which we skip one frame and each frame's intensity will be equal to the maximum of the previous frame and itself. A preliminary code is implemented in the `wrappers.py` file.\n",
    "* Episodic life: In case of *Breakout* losing takes 5 lives. To increase the chance of the model learning, we may end the game whenever one single life is lost.\n",
    "* Starting the game: In case of *Breakout* one should take a particular action to start the game. For example, they might need to press the `space` key in order for the game to start rolling. A wrapper is implemented to forcefully enter the game start action in the beginning. Indeed, in case this wrapper is not implemented, the agent can learn to take that action to gain higher reward; however, this will make the training time even longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0628852",
   "metadata": {},
   "source": [
    "After implementing or checking the wrappers in `src/wrappers.py` we can culminate all of the wrappers in the following class. A baseline is implemented for this wrapper but feel free to change it at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7dfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wrappers import PreprocessAtariObs, MaxAndSkipEnv, EpisodicLifeEnv, FireResetEnv, ClipRewardEnv, PreprocessAtariObs\n",
    "\n",
    "def PrimaryAtariWrap(env, clip_rewards=True, game):\n",
    "    \n",
    "    name = env.unwrapped.spec.id\n",
    "    \n",
    "    # This wrapper holds the same action for <skip> frames and outputs\n",
    "    # the maximal pixel value of 2 last frames (to handle blinking\n",
    "    # in some envs)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "    # This wrapper sends done=True when each life is lost\n",
    "    # (not all the 5 lives that are givern by the game rules).\n",
    "    # It should make easier for the agent to understand that losing is bad.\n",
    "    if name == \"BreakoutNoFrameskip-v4\":\n",
    "        EpisodicLifeEnv(env)\n",
    "\n",
    "    # This wrapper laucnhes the ball when an episode starts.\n",
    "    # Without it the agent has to learn this action, too.\n",
    "    # Actually it can but learning would take longer.\n",
    "    if name == \"BreakoutNoFrameskip-v4\":\n",
    "        env = FireResetEnv(env)\n",
    "\n",
    "    # This wrapper squeezes the rewards to range [-1, 1]\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "\n",
    "    # This wrapper is yours :)\n",
    "    env = PreprocessAtari(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2768e9",
   "metadata": {},
   "source": [
    "\n",
    "## Frame Buffer\n",
    "\n",
    "It is reasonable to assume that only one frame of the game does not hold information about the game dynamics. Hence, each observation should in fact contain multiple frames. There is a rule of thumb in these Atari games to consider buffering the last 4 frames of the game. In that case, each observation will be of form (H x W x C*F) where F represents the number of frames saved.\n",
    "\n",
    "To implement this, you may check the `src/framebuffer.py` file or change/implement your own frame buffer. With all of that done, we can run the following function to have an environment that has rich observations and reasonable rewards that provide the features necessary to train our DQN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framebuffer import FrameBuffer\n",
    "\n",
    "def make_env(clip_rewards=False, seed=None):\n",
    "    env = gym.make(ENV_NAME)  # create raw env\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    env = PrimaryAtariWrap(env, clip_rewards)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc491dc",
   "metadata": {},
   "source": [
    "### The DQN architecture\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
    "\n",
    "You can build any architecture you want, but for reference, here's something that will more or less work:\n",
    "\n",
    "<img src=\"fig/dqn_arch.png\" width=400 height=400 />\n",
    "\n",
    "**IMPORTANT NOTE!** The pytorch library takes inputs as tensors of size `[batch_size, channel_size, h, w]`; however, for visualization reasons, we have used numpy arrays of size `[h, w, channel_size]` up until now! Make sure to do the needed transformation before giving it convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4013915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, env, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.state_shape = (env.observation_space.shape[2],\n",
    "                            env.observation_space.shape[0],\n",
    "                            env.observation_space.shape[1])\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.conv1 = nn.Conv2d(self.state_shape[0], 16, (3, 3), 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), 2)\n",
    "        self.linear = self.linear1 = nn.Linear(3136, self.n_actions)\n",
    "        \n",
    "        # TODO: one RELU for all?!\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Flatten = nn.Flatten()\n",
    "        \n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        # nn.Flatten() can be useful\n",
    "        #<YOUR CODE>\n",
    "        #self.layers = nn.Sequential(\n",
    "            \n",
    "        #)\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, h, w, 4 * c]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        state_t = state_t.transpose(3, 2)\n",
    "        state_t = state_t.transpose(2, 1)\n",
    "        state_t = self.ReLU(self.conv1(state_t))\n",
    "        state_t = self.ReLU(self.conv2(state_t))\n",
    "        state_t = self.ReLU(self.conv3(state_t))\n",
    "        qvalues = self.ReLU(self.linear(self.Flatten(state_t)))\n",
    "        \n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == self.n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d621b4",
   "metadata": {},
   "source": [
    "Run the agent to see if it encounters any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf379ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, visualize=False):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    # TODO: your code here!\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below to see if the network can run a policy\n",
    "\n",
    "evaluate(env, agent, n_games=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b83e96",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "The common practice in DQN literature consists of using an experience replay buffer in which tuples of form $(s, a, r, s')$ are stored. In each epoch, a random batch of this buffer will be given to the network for training. \n",
    "\n",
    "In this section, we will implement this buffer; the interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for batch_size random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is shamelessly stolen from\n",
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        # fill the data cyclically, if the list is not yet complete append it\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return (\n",
    "            np.array(obses_t),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(obses_tp1),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [\n",
    "            random.randint(0, len(self._storage) - 1)\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "        return self._encode_sample(idxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26163fd0",
   "metadata": {},
   "source": [
    "Implement the function `play_and_record` in which we play the game for a number of steps and add the steps into an experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6819676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for i in range(n_steps):\n",
    "        # TODO: implement this section\n",
    "            \n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649cc7e",
   "metadata": {},
   "source": [
    "Run some simple sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb006c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
    "                   1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\"\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
    "                                 \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done should be strictly True or False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9131c23",
   "metadata": {},
   "source": [
    "### Target Networks\n",
    "\n",
    "DQN is basically a policy iteration method in which we both approximate the $Q$ values and improve them as we go. This might cause any errors in estimating to propagate and create unwanted difficulties. To address this issue we use **Target Networks**. Recall the main Q-Learning framework:\n",
    "\n",
    "$$Q(s, a; \\Theta) := Q(s, a; \\Theta) + \\eta \\big(R(s, a) + \\gamma \\times \\max_{a'} \\hat{Q}(s, a'; \\Theta')  - Q(s, a; \\Theta)\\big)$$\n",
    "\n",
    "In this framework we approximate the $\\hat{Q}(s, a'; \\Theta')$ values using a network parameterized by $\\Theta'$ and this is the actual target network. After certain frequencies we load the current network into the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(env, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500c3ab",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "Here we will finally implement the Q-Learning procedure. Here's the process:\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "\n",
    "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
    "\n",
    "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef092912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = <YOUR CODE>\n",
    "\n",
    "    assert next_state_values.dim(\n",
    "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10559dba",
   "metadata": {},
   "source": [
    "Sanity checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
    "    ), \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946cf59",
   "metadata": {},
   "source": [
    "## Main Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf843130",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = #<your favourite random seed>\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_shape, n_actions).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36f656",
   "metadata": {},
   "source": [
    "The following are the hyperparameters for the learning process. Feel free to change them as you will; although some default values are assigned to them currently.\n",
    "\n",
    "**IMPORTANT NOTE!** Since the training takes time, you may (and should!) save your model with some frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78174d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 16\n",
    "total_steps = 3 * 10**6\n",
    "decay_steps = 10**6\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 50\n",
    "refresh_target_network_freq = 5000\n",
    "eval_freq = 5000\n",
    "\n",
    "max_grad_norm = 50\n",
    "\n",
    "n_lives = 5\n",
    "\n",
    "save_freq = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5542f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS TO MAINTAIN THE HISTORY!\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "state = env.reset()\n",
    "for step in trange(step, total_steps + 1):\n",
    "    if not utils.is_enough_ram():\n",
    "        print('less that 100 Mb RAM available, freezing')\n",
    "        print('make sure everythin is ok and make KeyboardInterrupt to continue')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "    # play\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "    # train\n",
    "    # TODO: sample batch_size of data from experience replay>\n",
    "\n",
    "    loss = # TODO: compute TD loss\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if (step + 1) % save_freq == 0:\n",
    "        torch.save(agent, 'saved-models/temporary_saved_agent.pth')\n",
    "    \n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        <YOUR CODE>\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        mean_rw_history.append(evaluate(\n",
    "            make_env(clip_rewards=True, seed=step), agent, n_games=3 * n_lives, greedy=True)\n",
    "        )\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [make_env(seed=step).reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward per life\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Initial state V\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for step in trange(step, total_steps + 1):\n",
    "    if not is_enough_ram():\n",
    "        print('less that 100 Mb RAM available, freezing')\n",
    "        print('make sure everythin is ok and make KeyboardInterrupt to continue')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "    #print(\"epsilon = {}\".format(agent.epsilon))\n",
    "    # play\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "    #print(\"Played!\")\n",
    "    # train\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
    "    loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device)\n",
    "    #print(\"Calculated Loss\")\n",
    "    #<YOUR CODE: sample batch_size of data from experience replay>\n",
    "\n",
    "    #loss = <YOUR CODE: compute TD loss>\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    #print(\"Backprop done!\")\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        #target_network = DQNAgent(env).to(device)\n",
    "        target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        env = gym.make(ENV_NAME)\n",
    "        env.seed(seed)\n",
    "        env = PreprocessAtariObs(env)\n",
    "        env = FrameBuffer(env)\n",
    "        print(\"Started evaluating\")\n",
    "        print(\"start rw history\")\n",
    "        n_lives = 1\n",
    "        mean_rw_history.append(evaluate(\n",
    "            env, agent, n_games=3 * n_lives, greedy=True)\n",
    "        )\n",
    "        \n",
    "        env = gym.make(ENV_NAME)\n",
    "        env.seed(seed)\n",
    "        env = PreprocessAtariObs(env)\n",
    "        env = FrameBuffer(env)\n",
    "        print(\"Start init 'q' values\")\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [env.reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward per life\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Initial state V\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2ed1f",
   "metadata": {},
   "source": [
    "## How to interpret plots:\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* **TD loss** is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* **grad norm** just shows the intensivity of training. Not ok is growing to values of about 100 (or maybe even 50) though it depends on network architecture.\n",
    "* **mean reward** is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes about 40k steps to \"warm up\" agent before it starts to get better.\n",
    "* **Initial state V** is the expected discounted reward for episode in the oppinion of the agent. It should behave more smoothly than **mean reward**. It should get higher over time but sometimes can experience drawdowns because of the agaent's overestimates.\n",
    "* **buffer size** - this one is simple. It should go up and cap at max size.\n",
    "* **epsilon** - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - it means you need to increase epsilon. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Smoothing of plots is done with a gaussian kernel\n",
    "\n",
    "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
    "\n",
    "**Training will take time.** A lot of it actually. Probably you will not see any improvment during first **150k** time steps (note that by default in this notebook agent is evaluated every 5000 time steps).\n",
    "\n",
    "But hey, long training time isn't _that_ bad:\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd649c",
   "metadata": {},
   "source": [
    "## See how the model works and Assignment Submission\n",
    "\n",
    "To submit the assignment, you should make sure to have the following items checked:\n",
    "* Each of the blocks provided should run without error.\n",
    "* Run the training and save your model in `<STD_ID>_DQN_<ENV_NAME>.pth` format and upload it alongside the assignment.\n",
    "* The history of your plots **from the first epoch until the end**. \n",
    "* You should also save your output as a gif `<STD_ID>_DQN_<ENV_NAME>.gif` and upload it alongside your files.\n",
    "\n",
    "You can use the following codes to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_model(seed, agent, n_games=10, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.seed(seed)\n",
    "    env = PreprocessAtariObs(env)\n",
    "    env = FrameBuffer(env)\n",
    "    state_shape = env.observation_space.shape\n",
    "    n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "    actions_history = []\n",
    "    for _ in trange(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        current_session_actions = []\n",
    "        for t in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0]\n",
    "            current_session_actions.append(action)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(reward)\n",
    "        actions_history.append(current_session_actions)\n",
    "    t = np.array(rewards)\n",
    "    t_max = np.sort(t)[-5]\n",
    "    ids = rewards >= t_max\n",
    "    print(\"Average reward: {}\".format(np.mean(rewards)))\n",
    "    print(ids)\n",
    "    time.sleep(2)\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.seed(seed)\n",
    "    for i in range(n_games):\n",
    "        s = env.reset()\n",
    "        for j, a in enumerate(actions_history[i]):\n",
    "            actions_history.append(action)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            if ids[i] and (j + 1) % 2 == 0:\n",
    "                clear_output(True)\n",
    "                screen = env.render(mode='rgb_array')\n",
    "                plt.imshow(screen, aspect='auto')\n",
    "                plt.show()\n",
    "                # TODO: Adjust for controling frame speed\n",
    "                time.sleep(0.05)\n",
    "            if done:\n",
    "                break\n",
    "    return np.mean(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
